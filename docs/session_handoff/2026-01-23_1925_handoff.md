# Session Handoff: Docker Networking & Data Ingestion Fixes

Read this file to continue.

## üö© Critical Issues Encountered & Status

### 1. Docker Connectivity Crash (FIXED)
-   **Symptom**: Containers were crashing immediately with `connect: network is unreachable`.
-   **Root Cause**: Docker Desktop on Windows has issues routing IPv6 traffic. The Supabase direct DSN resolved to an IPv6 address that the container couldn't reach.
-   **Fix Applied**: Switched `.env.local` to use the **Supavisor Connection Pooler** (Session Mode, port 5432), which forces an IPv4 connection.
-   **Verification**: "pizza milan" job container exited with code 0 (success) and logs showed "scrapemate exited" without network errors.

### 2. Scraper File Permission Error (FIXED)
-   **Symptom**: Scraper output "open /results.csv: is a directory".
-   **Root Cause**: When mounting a non-existent file path (`-v host/path:/container/path`), Docker auto-creates it as a **directory** on the host. The scraper then tried to open this directory as a file and failed.
-   **Fix Applied**: Modified `docker.ts` to `writeFile` (create an empty `results.csv`) *before* spawning the container.
-   **Verification**: Manual manual verification showed `results.csv` is now a file.

### 3. Missing Leads in Supabase (PENDING)
-   **Symptom**: Jobs run successfully, `results.csv` (likely) has data, but **0 leads** appear in the UI.
-   **Root Cause**: The upstream `google-maps-scraper` **does not support** the `-job-id` and `-user-id` flags we were passing. We removed them to prevent the scraper from crashing on unknown flags. Without these flags, the scraper cannot insert rows into Supabase with the correct foreign keys.
-   **Solution Plan**: We must implement a "Post-Processing" step. Instead of the scraper writing to DB, we will let it write to `results.csv`, then our app will parse that file and insert the leads into Supabase, linking them to the `job_id`.

## üõ†Ô∏è Next Code Changes Required

1.  **Modify `src/lib/docker.ts`**:
    -   Restore `--rm` flag (currently commented out for debug).
    -   Force `-debug` to true in `spawnScraperContainer` (optional, for stability).
    -   Implement `processJobResults(jobId: string)`:
        -   Locate `results.csv` in `os.tmpdir()/lcc-jobs/<jobId>`.
        -   Read and parse the CSV.
        -   Insert into Supabase `results` table with `job_id` and `user_id`.

2.  **Create Sync API**:
    -   `POST /api/jobs/[id]/sync` -> Calls `processJobResults`.

3.  **Update UI**:
    -   Poll looking for status changes or add a manual "Sync" button (or automatic polling).

## Project State
-   **Location**: `d:\Websites\GMaps_scraper_gosom\leads-command-center`
-   **Env**: `.env.local` is updated with IPv4 DSN.
-   **Docs**: `implementation_plan.md` has the detailed plan.
