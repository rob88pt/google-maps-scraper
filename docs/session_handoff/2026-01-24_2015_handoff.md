# Session Handoff: Google Maps Scraper Monorepo

Read this file to continue where the previous session left off.

## Project Structure (NEW)
The project has been converted to a **Monorepo** at `d:\Websites\GMaps_scraper_gosom`.
- `leads-command-center/`: Web App
- `source/`: Scraper (managed via Git Subtree)
- `docs/`: Unified documentation

## Context Files to Read First
1. `docs/active_context.md` - current state overview
2. `docs/fork_strategy.md` - **CRITICAL**: Read this to understand how to maintain the repo and sync upstream changes.
3. `docs/changelog_summary.md`

## What Was Done This Session
- **Reverted** unreliable review image scraping code (DOM attempts proved fragile).
- **Consolidated** Git repositories into a single unified root repo.
- **Configured** `upstream-scraper` remote for easier updates.
- **Fixed** VS Code "mess" by removing nested `.git` folders from backups.
- **Updated** documentation to reflect the new architecture.

## What's Left to Do
- [ ] **Clean up** 18GB backup folder (reclaim space). The recursive delete command was triggered but should be verified.
- [ ] **Performance**: Add timeout safeguard to review extraction in `place.go`.
- [ ] **Limits**: Add page limit to RPC review fetcher in `reviews.go`.

## Key Decisions Made
- **Monorepo**: Single git history for simpler versioning of the full stack.
- **Reversion**: Prioritized stability over fragile feature (review images).
- **Subtree**: Strategy chosen for managing the scraper dependency.

## How to Verify Current State
```powershell
# Check git status
git status

# Check remotes (should see upstream-scraper)
git remote -v
```

## Suggested First Action
Verify the 18GB backup cleanup is finished, then proceed with the Performance Safeguards (timeout implementation).
